{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to create process using 'C:\\Users\\nabin shrestha\\.conda\\envs\\Major_project\\python.exe \"C:\\Users\\nabin shrestha\\.conda\\envs\\Major_project\\Scripts\\conda-script.py\" install editdistance'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import editdistance as ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [\"हात\", \"हार\", \"सार\", \"धुनुहोस्\", \"स्वस्थ\", \"रहनुहोस्\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transliteration_map = {\n",
    "    \"ह\": \"ha\", \"ा\": \"a\", \"त\": \"t\", \"र\": \"r\", \"स\": \"sa\", \"ध\": \"dhu\", \"ु\": \"u\",\n",
    "    \"न\": \"n\", \"हु\": \"hu\", \"स्व\": \"sw\", \"स्\": \"s\", \"थ\": \"th\", \"र\": \"r\", \"ह\": \"ha\", \"नु\": \"nu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transliterate(word):\n",
    "    transliterated = ''.join(transliteration_map.get(char, char) for char in word)\n",
    "    return transliterated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(word, vocabulary, max_distance=2):\n",
    "    candidates = []\n",
    "    transliterated_word = transliterate(word)\n",
    "    for vocab_word in vocabulary:\n",
    "        transliterated_vocab_word = transliterate(vocab_word)\n",
    "        distance = ed.eval(transliterated_word, transliterated_vocab_word)\n",
    "        if distance <= max_distance:\n",
    "            candidates.append((vocab_word, distance))\n",
    "    return sorted(candidates, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_context(candidates, context_word):\n",
    "    # Here we assume \"धुनुहोस्\" (wash) context for demonstration\n",
    "    context_related = [\"हात\", \"हार\"]  # Words that make sense in the context of washing\n",
    "    filtered_candidates = [word for word in candidates if word[0] in context_related]\n",
    "    return filtered_candidates if filtered_candidates else candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test example\n",
    "erroneous_word = \"हार\"\n",
    "context_word = \"धुनुहोस्\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Candidates: [('हार', 0), ('हात', 1), ('सार', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Generate candidates\n",
    "candidates = generate_candidates(erroneous_word, vocabulary)\n",
    "print(\"Generated Candidates:\", candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Candidates: [('हार', 0), ('हात', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Context filtering\n",
    "filtered_candidates = filter_by_context(candidates, context_word)\n",
    "print(\"Filtered Candidates:\", filtered_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################Senteces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Sentence: हात धुनुहोस् र स्वस्थ रहनुहोस्\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocabulary = [\"हात\", \"हार\", \"सार\", \"धुनुहोस्\", \"स्वस्थ\", \"रहनुहोस्\"]\n",
    "\n",
    "def filter_by_context(candidates, context_words):\n",
    "    # Simple context filtering: prioritize candidates based on context\n",
    "    context_related = context_words\n",
    "    filtered_candidates = [word for word in candidates if word[0] in context_related]\n",
    "    return filtered_candidates if filtered_candidates else candidates\n",
    "\n",
    "def correct_sentence(sentence, vocabulary, context_words):\n",
    "    words = sentence.split()\n",
    "    corrected_sentence = []\n",
    "    for word in words:\n",
    "        # Generate candidates for the word\n",
    "        candidates = generate_candidates(word, vocabulary)\n",
    "        # Filter candidates based on context\n",
    "        filtered_candidates = filter_by_context(candidates, context_words)\n",
    "        # Select the best candidate (here, the first one after filtering)\n",
    "        best_candidate = filtered_candidates[0][0] if filtered_candidates else word\n",
    "        corrected_sentence.append(best_candidate)\n",
    "    return ' '.join(corrected_sentence)\n",
    "\n",
    "# Test example\n",
    "erroneous_sentence = \"हार धुनुहोस् र स्वस्थ रहनुहोस्।\"  # \"Wash your necklace to stay healthy.\"\n",
    "context_words = [\"हात\", \"धुनुहोस्\", \"स्वस्थ\", \"रहनुहोस्\"]  # Words that fit in the context of washing and health\n",
    "\n",
    "corrected_sentence = correct_sentence(erroneous_sentence, vocabulary, context_words)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Integrating word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the file\n",
    "# file_path = 'D:/Major Project/enironment/unique_words.txt'\n",
    "file_path = \"D:/Major Project/enironment/implementation_p/lamsal_vocabulary.txt\"\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    # Read all lines into a list\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Strip whitespace characters like `\\n` at the end of each line\n",
    "lines = [line.strip() for line in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515851"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nepali_stop_words = stopwords.words('nepali')\n",
    "len(nepali_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['छ', 'र', 'पनि', 'छन्', 'लागि', 'भएको', 'गरेको', 'भने', 'गर्न', 'गर्ने', 'हो', 'तथा', 'यो', 'रहेको', 'उनले', 'थियो', 'हुने', 'गरेका', 'थिए', 'गर्दै', 'तर', 'नै', 'को', 'मा', 'हुन्', 'भन्ने', 'हुन', 'गरी', 'त', 'हुन्छ', 'अब', 'के', 'रहेका', 'गरेर', 'छैन', 'दिए', 'भए', 'यस', 'ले', 'गर्नु', 'औं', 'सो', 'त्यो', 'कि', 'जुन', 'यी', 'का', 'गरि', 'ती', 'न', 'छु', 'छौं', 'लाई', 'नि', 'उप', 'अक्सर', 'आदि', 'कसरी', 'क्रमशः', 'चाले', 'अगाडी', 'अझै', 'अनुसार', 'अन्तर्गत', 'अन्य', 'अन्यत्र', 'अन्यथा', 'अरु', 'अरुलाई', 'अर्को', 'अर्थात', 'अर्थात्', 'अलग', 'आए', 'आजको', 'ओठ', 'आत्म', 'आफू', 'आफूलाई', 'आफ्नै', 'आफ्नो', 'आयो', 'उदाहरण', 'उनको', 'उहालाई', 'एउटै', 'एक', 'एकदम', 'कतै', 'कम से कम', 'कसै', 'कसैले', 'कहाँबाट', 'कहिलेकाहीं', 'का', 'किन', 'किनभने', 'कुनै', 'कुरा', 'कृपया', 'केही', 'कोही', 'गए', 'गरौं', 'गर्छ', 'गर्छु', 'गर्नुपर्छ', 'गयौ', 'गैर', 'चार', 'चाहनुहुन्छ', 'चाहन्छु', 'चाहिए', 'छू', 'जताततै', 'जब', 'जबकि', 'जसको', 'जसबाट', 'जसमा', 'जसलाई', 'जसले', 'जस्तै', 'जस्तो', 'जस्तोसुकै', 'जहाँ', 'जान', 'जाहिर', 'जे', 'जो', 'ठीक', 'तत्काल', 'तदनुसार', 'तपाईको', 'तपाई', 'पर्याप्त', 'पहिले', 'पहिलो', 'पहिल्यै', 'पाँच', 'पाँचौं', 'तल', 'तापनी', 'तिनी', 'तिनीहरू', 'तिनीहरुको', 'तिनिहरुलाई', 'तिमी', 'तिर', 'तीन', 'तुरुन्तै', 'तेस्रो', 'तेस्कारण', 'पूर्व', 'प्रति', 'प्रतेक', 'प्लस', 'फेरी', 'बने', 'त्सपछि', 'त्सैले', 'त्यहाँ', 'थिएन', 'दिनुभएको', 'दिनुहुन्छ', 'दुई', 'देखि', 'बरु', 'बारे', 'बाहिर', 'देखिन्छ', 'देखियो', 'देखे', 'देखेको', 'देखेर', 'दोस्रो', 'धेरै', 'नजिकै', 'नत्र', 'नयाँ', 'निम्ति', 'बाहेक', 'बीच', 'बीचमा', 'भन', 'निम्न', 'निम्नानुसार', 'निर्दिष्ट', 'नौ', 'पक्का', 'पक्कै', 'पछि', 'पछिल्लो', 'पटक', 'पर्छ', 'पर्थ्यो', 'भन्छन्', 'भन्', 'भन्छु', 'भन्दा', 'भन्नुभयो', 'भर', 'भित्र', 'भित्री', 'म', 'मलाई', 'मात्र', 'माथि', 'मुख्य', 'मेरो', 'यति', 'यथोचित', 'यदि', 'यद्यपि', 'यसको', 'यसपछि', 'यसबाहेक', 'यसरी', 'यसो', 'यस्तो', 'यहाँ', 'यहाँसम्म', 'या', 'रही', 'राखे', 'राख्छ', 'राम्रो', 'रूप', 'लगभग', 'वरीपरी', 'वास्तवमा', 'बिरुद्ध', 'बिशेष', 'सायद', 'शायद', 'संग', 'संगै', 'सक्छ', 'सट्टा', 'सधै', 'सबै', 'सबैलाई', 'समय', 'सम्भव', 'सम्म', 'सही', 'साँच्चै', 'सात', 'साथ', 'साथै', 'सारा', 'सोही', 'स्पष्ट', 'हरे', 'हरेक']\n"
     ]
    }
   ],
   "source": [
    "print(nepali_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #remove stop words\n",
    "uniq_vocab = [word for word in lines if word not in nepali_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515619"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uniq_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['गरे']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_vocab[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"हात\" in uniq_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load vectors\n",
    "model = KeyedVectors.load_word2vec_format(\"C:/Users/nabin shrestha/Downloads/nepali_embeddings_word2vec.txt\", binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['पनि', 'भए', 'गरे', '‘', '’', 'छन्', 'गर्न', 'लागि', 'उन', 'नेपाल', 'गर्ने', 'थियो', 'तथा', 'रहे', 'सरकार', 'यस', 'गरि', 'हुने', 'बताए', 'तर', 'थिए', 'काम', 'वर्ष', 'एक', 'छैन', 'कु', 'आए', 'क्षेत्र', 'भन्ने', 'त्यस', 'निर्माण', 'जना', 'गरी', 'अनुसार', 'नेपाली', 'दिन', 'प्रतिक्रिया', 'स्थानीय', 'सबै', 'हजार', 'हुन्छ', 'केही', 'प्रदेश', 'गर्', 'प्रहरी', 'नयाँ', 'हामी', 'सय', 'विकास', 'जिल्ला', 'कारण', 'समय', 'उनी', 'कुरा', 'देखि', 'दुई', 'नाम', 'मात्र', 'रुप', 'आफ्नो', 'रूप', 'लाख', 'धेरै', 'कार्यालय', 'अहि', 'अवस्था', 'समिति', 'कार्यक्रम', 'अघि', 'घर', 'बढी', 'गरेर', 'दिए', 'चित्र', 'प्रमुख', 'विषय', 'खेल', 'पुनर्उत्पादन', 'सडक', 'सुरु', 'समेत', 'गर्नु', 'क्याप्चा', 'ईमेल', 'काठमाडौं', 'समस्या', 'पहिलो', 'प्रक्रिया', 'अध्यक्ष', 'त्यो', 'देश', 'पार्टी', 'नि', 'कानुन', 'भारत', 'अन्तर्राष्ट्रिय', 'रुपैयाँ', 'भयो', 'प्रधानमन्त्री', 'परे']\n"
     ]
    }
   ],
   "source": [
    "# List out the vocabulary\n",
    "vocabulary = list(model.key_to_index.keys())\n",
    "\n",
    "# Print the first 10 words in the vocabulary as a sample\n",
    "print(vocabulary[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515851"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"lamsal_vocabulary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for word in vocabulary:\n",
    "#         f.write(word + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Sentence: हुर धुनुहोस् र स्वश्थ रहनुहोस्।\n"
     ]
    }
   ],
   "source": [
    "import editdistance\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load your pre-trained Word2Vec model\n",
    "# Assuming you have a model saved as 'nepali_word2vec.model'\n",
    "# model = Word2Vec.load('D:/Major Project/enironment/models/final_word2vec_model_3.model')\n",
    "# model = Word2Vec.load(\"C:/Users/nabin shrestha/Downloads/nepali_word2vec_whole_corpus.model\")\n",
    "\n",
    "# Sample vocabulary (in a real scenario, this would be much larger)\n",
    "# vocabulary = [\"हात\", \"हार\", \"सार\", \"धुनुहोस्\", \"स्वस्थ\", \"रहनुहोस्\", \"साबुन\", \"पानी\"]\n",
    "vocabulary = lines\n",
    "\n",
    "# Sample transliteration map (simplified for demonstration)\n",
    "transliteration_map = {\n",
    "    'अ': 'a', 'आ': 'aa', 'इ': 'i', 'ई': 'ee', 'उ': 'u', 'ऊ': 'oo', 'ऋ': 'ri', 'ए': 'e', 'ऐ': 'ai', 'ओ': 'o', 'औ': 'au',\n",
    "    'अं': 'am', 'अः': 'ah', 'ा': 'a', 'ि': 'i', 'ी': 'ee', 'ु': 'u', 'ू': 'oo', 'ृ': 'ri', 'े': 'e', 'ै': 'ai', 'ो': 'o', 'ौ': 'au',\n",
    "    'क': 'ka', 'ख': 'kha', 'ग': 'ga', 'घ': 'gha', 'ङ': 'nga', 'च': 'cha', 'छ': 'chha', 'ज': 'ja', 'झ': 'jha', 'ञ': 'nya',\n",
    "    'ट': 'ta', 'ठ': 'tha', 'ड': 'da', 'ढ': 'dha', 'ण': 'na', 'त': 'ta', 'थ': 'tha', 'द': 'da', 'ध': 'dha', 'न': 'na',\n",
    "    'प': 'pa', 'फ': 'pha', 'ब': 'ba', 'भ': 'bha', 'म': 'ma', 'य': 'ya', 'र': 'ra', 'ल': 'la', 'व': 'va', 'श': 'sha',\n",
    "    'ष': 'sha', 'स': 'sa', 'ह': 'ha', 'क्ष': 'kshya', 'त्र': 'tra', 'ज्ञ': 'gya', 'ऽ': \"'\", 'ँ': 'n', 'ं': 'm', 'ः': 'h',\n",
    "    '़': '', '्': '', '०': '0', '१': '1', '२': '2', '३': '3', '४': '4', '५': '5', '६': '6', '७': '7', '८': '8', '९': '9',\n",
    "    'ॐ': 'om', 'क़': 'qa', 'ख़': 'kha', 'ग़': 'gha', 'ज़': 'za', 'ड़': 'ra', 'ढ़': 'rha', 'फ़': 'fa', 'य़': 'ya'\n",
    "}\n",
    "\n",
    "\n",
    "def transliterate(word):\n",
    "    # transliterated = ''.join(transliteration_map.get(char, char) for char in word)\n",
    "    transliterated = word\n",
    "    return transliterated\n",
    "\n",
    "def generate_candidates(word, vocabulary, max_distance=2):\n",
    "    candidates = []\n",
    "    transliterated_word = transliterate(word)\n",
    "    for vocab_word in vocabulary:\n",
    "        transliterated_vocab_word = transliterate(vocab_word)\n",
    "        distance = editdistance.eval(transliterated_word, transliterated_vocab_word)\n",
    "        if distance <= max_distance:\n",
    "            candidates.append((vocab_word, distance))\n",
    "    return sorted(candidates, key=lambda x: x[1])\n",
    "\n",
    "\n",
    "# Modifying such that first character remain ssame while generating candidates\n",
    "# def generate_candidates(word, vocabulary, max_distance=1):\n",
    "#     candidates = []\n",
    "#     transliterated_word = transliterate(word)\n",
    "#     first_char = transliterated_word[0]  # Get the first character of the transliterated word\n",
    "#     for vocab_word in vocabulary:\n",
    "#         transliterated_vocab_word = transliterate(vocab_word)\n",
    "#         if transliterated_vocab_word.startswith(first_char):  # Check if first character matches\n",
    "#             distance = editdistance.eval(transliterated_word, transliterated_vocab_word)\n",
    "#             if distance <= max_distance:\n",
    "#                 candidates.append((vocab_word, distance))\n",
    "#     return sorted(candidates, key=lambda x: x[1])\n",
    "\n",
    "# Modying such that first and last charcter should match\n",
    "# def generate_candidates(word, vocabulary, max_distance=2):\n",
    "#     candidates = []\n",
    "#     transliterated_word = transliterate(word)\n",
    "#     first_char = transliterated_word[0]  # Get the first character of the transliterated word\n",
    "#     last_char = transliterated_word[-1]  # Get the last character of the transliterated word\n",
    "#     for vocab_word in vocabulary:\n",
    "#         transliterated_vocab_word = transliterate(vocab_word)\n",
    "#         if (transliterated_vocab_word.startswith(first_char) and\n",
    "#             transliterated_vocab_word.endswith(last_char)):  # Check if first and last characters match\n",
    "#             distance = editdistance.eval(transliterated_word, transliterated_vocab_word)\n",
    "#             if distance <= max_distance:\n",
    "#                 candidates.append((vocab_word, distance))\n",
    "#     return sorted(candidates, key=lambda x: x[1])\n",
    "\n",
    "# modify such  that first,last character as well as length should also match\n",
    "def generate_candidates(word, vocabulary, max_distance=2):\n",
    "    candidates = []\n",
    "    transliterated_word = transliterate(word)\n",
    "    first_char = transliterated_word[0]  # Get the first character of the transliterated word\n",
    "    last_char = transliterated_word[-1]  # Get the last character of the transliterated word\n",
    "    word_length = len(transliterated_word)  # Get the length of the transliterated word\n",
    "    for vocab_word in vocabulary:\n",
    "        transliterated_vocab_word = transliterate(vocab_word)\n",
    "        if (len(transliterated_vocab_word) == word_length and\n",
    "            transliterated_vocab_word.startswith(first_char) and\n",
    "            transliterated_vocab_word.endswith(last_char)):  # Check length and first/last characters match\n",
    "            distance = editdistance.eval(transliterated_word, transliterated_vocab_word)\n",
    "            if distance <= max_distance:\n",
    "                candidates.append((vocab_word, distance))\n",
    "    return sorted(candidates, key=lambda x: x[1])\n",
    "\n",
    "\n",
    "\n",
    "# def filter_by_context(candidates, context_words, model):\n",
    "#     filtered_candidates = []\n",
    "#     for candidate, distance in candidates:\n",
    "#         candidate_score = 0\n",
    "#         for context_word in context_words:\n",
    "#             if context_word in model.wv and candidate in model.wv:\n",
    "#                 candidate_score += model.wv.similarity(context_word, candidate)\n",
    "#         filtered_candidates.append((candidate, distance, candidate_score))\n",
    "    \n",
    "#     filtered_candidates.sort(key=lambda x: (-x[2], x[1]))  # Sort by score (descending) and distance (ascending)\n",
    "#     return filtered_candidates if filtered_candidates else candidates\n",
    "\n",
    "#rabindra lamsal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filter_by_context(candidates, context_words, model):\n",
    "    filtered_candidates = []\n",
    "    for candidate, distance in candidates:\n",
    "        candidate_score = 0\n",
    "        for context_word in context_words:\n",
    "            if context_word in model.key_to_index and candidate in model.key_to_index:\n",
    "                candidate_score += model.similarity(context_word, candidate)\n",
    "        filtered_candidates.append((candidate, distance, candidate_score))\n",
    "    \n",
    "    # Sort by score (descending) and distance (ascending)\n",
    "    filtered_candidates.sort(key=lambda x: (-x[2], x[1]))\n",
    "    return filtered_candidates if filtered_candidates else candidates\n",
    "\n",
    "\n",
    "def correct_sentence(sentence, vocabulary, model):\n",
    "    words = sentence.split()\n",
    "    corrected_sentence = []\n",
    "    for i, word in enumerate(words):\n",
    "        context_words = [w for j, w in enumerate(words) if j != i]\n",
    "        # Generate candidates for the word\n",
    "        candidates = generate_candidates(word, vocabulary)\n",
    "        # Filter candidates based on context\n",
    "        filtered_candidates = filter_by_context(candidates, context_words, model)\n",
    "        # Select the best candidate (here, the first one after filtering)\n",
    "        best_candidate = filtered_candidates[0][0] if filtered_candidates else word\n",
    "        corrected_sentence.append(best_candidate)\n",
    "    return ' '.join(corrected_sentence)\n",
    "\n",
    "# Test example\n",
    "erroneous_sentence =\"हार धुनुहोस् र स्वस्थ रहनुहोस्।\"\n",
    "# erroneous_sentence = \"फुल खान्छु \"#\"फुल टिप्छु\"\n",
    "# erroneous_sentence = \"बाघले आफ्नो घर बनाए\"\n",
    "\n",
    "corrected_sentence = correct_sentence(erroneous_sentence, vocabulary, model)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Word: म, Context Words: ['फुल', 'टिप्छु']\n",
      "Current Word: फुल, Context Words: ['म', 'टिप्छु']\n",
      "Current Word: टिप्छु, Context Words: ['म', 'फुल']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"म फुल टिप्छु\"\n",
    "words = sentence.split()\n",
    "corrected_sentence = []\n",
    "for i, word in enumerate(words):\n",
    "    context_words = [w for j, w in enumerate(words) if j != i]\n",
    "    print(f\"Current Word: {word}, Context Words: {context_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'धुनुहोस्'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77652067"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.wv.similarity('एमाले','काँग्रेस')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT TRANSILIRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'wv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Test example\u001b[39;00m\n\u001b[0;32m     37\u001b[0m erroneous_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mनेपाल सरकारले नाइँ योजना पारित गर्भको\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# \"Wash your necklace to stay healthy.\"\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m corrected_sentence \u001b[38;5;241m=\u001b[39m \u001b[43mcorrect_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43merroneous_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrected Sentence:\u001b[39m\u001b[38;5;124m\"\u001b[39m, corrected_sentence)\n",
      "Cell \u001b[1;32mIn[26], line 30\u001b[0m, in \u001b[0;36mcorrect_sentence\u001b[1;34m(sentence, vocabulary, model)\u001b[0m\n\u001b[0;32m     28\u001b[0m candidates \u001b[38;5;241m=\u001b[39m generate_candidates(word, vocabulary)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Filter candidates based on context\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m filtered_candidates \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_by_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Select the best candidate (here, the first one after filtering)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m best_candidate \u001b[38;5;241m=\u001b[39m filtered_candidates[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m filtered_candidates \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "Cell \u001b[1;32mIn[26], line 15\u001b[0m, in \u001b[0;36mfilter_by_context\u001b[1;34m(candidates, context_words, model)\u001b[0m\n\u001b[0;32m     13\u001b[0m candidate_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m context_word \u001b[38;5;129;01min\u001b[39;00m context_words:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m candidate \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mwv:\n\u001b[0;32m     16\u001b[0m         candidate_score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39msimilarity(context_word, candidate)\n\u001b[0;32m     17\u001b[0m filtered_candidates\u001b[38;5;241m.\u001b[39mappend((candidate, distance, candidate_score))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyedVectors' object has no attribute 'wv'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_candidates(word, vocabulary, max_distance=2):\n",
    "    candidates = []\n",
    "    for vocab_word in vocabulary:\n",
    "        if len(vocab_word) == len(word) and vocab_word[0] == word[0] and vocab_word[-1] == word[-1]:\n",
    "            distance = editdistance.eval(word, vocab_word)\n",
    "            if distance <= max_distance:\n",
    "                candidates.append((vocab_word, distance))\n",
    "    return sorted(candidates, key=lambda x: x[1])\n",
    "\n",
    "def filter_by_context(candidates, context_words, model):\n",
    "    filtered_candidates = []\n",
    "    for candidate, distance in candidates:\n",
    "        candidate_score = 0\n",
    "        for context_word in context_words:\n",
    "            if context_word in model.wv and candidate in model.wv:\n",
    "                candidate_score += model.wv.similarity(context_word, candidate)\n",
    "        filtered_candidates.append((candidate, distance, candidate_score))\n",
    "    \n",
    "    filtered_candidates.sort(key=lambda x: (-x[2], x[1]))  # Sort by score (descending) and distance (ascending)\n",
    "    return filtered_candidates if filtered_candidates else candidates\n",
    "\n",
    "def correct_sentence(sentence, vocabulary, model):\n",
    "    words = sentence.split()\n",
    "    corrected_sentence = []\n",
    "    for i, word in enumerate(words):\n",
    "        context_words = [w for j, w in enumerate(words) if j != i]\n",
    "        # Generate candidates for the word\n",
    "        candidates = generate_candidates(word, vocabulary)\n",
    "        # Filter candidates based on context\n",
    "        filtered_candidates = filter_by_context(candidates, context_words, model)\n",
    "        # Select the best candidate (here, the first one after filtering)\n",
    "        best_candidate = filtered_candidates[0][0] if filtered_candidates else word\n",
    "        corrected_sentence.append(best_candidate)\n",
    "    return ' '.join(corrected_sentence)\n",
    "\n",
    "# Test example\n",
    "erroneous_sentence = \"नेपाल सरकारले नाइँ योजना पारित गर्भको\"  # \"Wash your necklace to stay healthy.\"\n",
    "\n",
    "corrected_sentence = correct_sentence(erroneous_sentence, vocabulary, model)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################hit and trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('नीति', 0), ('नाति', 1), ('नीजि', 1), ('नीता', 1), ('निति', 1), ('नीतु', 1), ('नीलि', 1), ('नीती', 1), ('नीधि', 1), ('नीनि', 1), ('नीतू', 1), ('न्ति', 1), ('नेति', 1)]\n",
      "filtered candidates: [('नीति', 0, 1.533589355647564), ('निति', 1, 1.1541104230564088), ('नीती', 1, 0.8042844664305449), ('नीजि', 1, 0.2655605284962803), ('नीधि', 1, 0.19382217340171337), ('नीनि', 1, 0.18010213831439614), ('न्ति', 1, 0.11810791568132117), ('नेति', 1, 0.03966669738292694), ('नीतू', 1, -0.036910456605255604), ('नीता', 1, -0.0649286936968565), ('नीलि', 1, -0.19891158491373062), ('नीतु', 1, -0.2082822252996266), ('नाति', 1, -0.5409163143485785)]\n",
      "[('तथा', 0), ('तला', 1), ('तता', 1), ('तया', 1), ('तना', 1), ('तहा', 1), ('तगा', 1), ('तपा', 1), ('तखा', 1), ('तफा', 1), ('तवा', 1), ('तमा', 1), ('तथो', 1), ('तका', 1), ('तरा', 1), ('तजा', 1), ('तटा', 1), ('तदा', 1), ('तथँ', 1)]\n",
      "filtered candidates: [('तथा', 0, 1.6789884064346552), ('तटा', 1, 0.33042905014008284), ('तका', 1, 0.19932136498391628), ('तला', 1, 0.19717802572995424), ('तथँ', 1, 0.1931538674980402), ('तगा', 1, 0.17045043059624732), ('तवा', 1, 0.16647708136588335), ('तफा', 1, 0.15298837376758456), ('तना', 1, 0.0844947462901473), ('तजा', 1, 0.022535916534252465), ('तया', 1, 0.005983235314488411), ('तहा', 1, -0.010014279745519161), ('तदा', 1, -0.07275222224416211), ('तथो', 1, -0.07661247067153454), ('तमा', 1, -0.13306532055139542), ('तरा', 1, -0.1381868403404951), ('तखा', 1, -0.2236368302255869), ('तपा', 1, -0.4327481940272264), ('तता', 1, -0.5548366773873568)]\n",
      "[('कार्यक्रम', 0), ('कार्यत्रम', 1), ('कार्यव्रम', 1), ('कार्यक्तम', 1), ('कार्यक्रक', 1), ('कार्यक्रय', 1), ('कार्मक्रम', 1), ('कार्यम्रम', 1)]\n",
      "filtered candidates: [('कार्यक्रम', 0, 1.5721232471987605), ('कार्यत्रम', 1, 1.014479622244835), ('कार्यक्रक', 1, 0.5615798267535865), ('कार्यव्रम', 1, 0.5576549489051104), ('कार्यक्तम', 1, 0.36511162854731083), ('कार्यम्रम', 1, 0.2705722004175186), ('कार्यक्रय', 1, 0.1420577559620142), ('कार्मक्रम', 1, -0.3216930143535137)]\n",
      "[]\n",
      "filtered candidates: []\n",
      "[('उच्छ', 0), ('उच्च', 1), ('उठ्छ', 1), ('उड्छ', 1), ('उन्छ', 1)]\n",
      "filtered candidates: [('उच्छ', 0, 0.9063787693157792), ('उच्च', 1, 0.7830778621137142), ('उन्छ', 1, 0.4985618139617145), ('उठ्छ', 1, 0.36082220636308193), ('उड्छ', 1, -0.21507788402959704)]\n",
      "[('आर्थिक', 0), ('आर्थीक', 1), ('आर्टिक', 1), ('आस्थिक', 1), ('आर्जिक', 1)]\n",
      "filtered candidates: [('आर्थिक', 0, 1.649656344205141), ('आर्थीक', 1, 0.5318948579952121), ('आर्टिक', 1, 0.35648918617516756), ('आस्थिक', 1, 0.24236698122695088), ('आर्जिक', 1, -0.2737520672380924)]\n",
      "[('वृद्धि', 0), ('वृद्धा', 1), ('वृद्घि', 1), ('वृद्धी', 1), ('वृद्वि', 1), ('वुद्धि', 1), ('वृद्दि', 1), ('वृद्ध‚', 1)]\n",
      "filtered candidates: [('वृद्धि', 0, 1.389195078983903), ('वृद्घि', 1, 0.8056870591826737), ('वृद्वि', 1, 0.49198325583711267), ('वृद्दि', 1, 0.4026730335317552), ('वृद्धी', 1, 0.17918883120728424), ('वुद्धि', 1, 0.066124286968261), ('वृद्ध‚', 1, 0.025218247435986996), ('वृद्धा', 1, -0.5932549415156245)]\n",
      "[]\n",
      "filtered candidates: []\n",
      "[('स्टेप', 0), ('स्टेट', 1), ('स्टेज', 1), ('स्टेम', 1), ('स्टेन', 1), ('स्टेड', 1), ('स्टेक', 1), ('स्केप', 1), ('स्टेभ', 1), ('स्टाप', 1), ('स्टोप', 1), ('स्टेल', 1), ('स्टेस', 1), ('स्टेच', 1), ('स्टेफ', 1), ('स्टेउ', 1), ('स्टिप', 1), ('स्टेग', 1), ('स्टेर', 1)]\n",
      "filtered candidates: [('स्टेप', 0, 0.8124256860464811), ('स्टेम', 1, 0.5966735961847007), ('स्केप', 1, 0.4932358197402209), ('स्टेट', 1, 0.4085219237022102), ('स्टेक', 1, 0.39750384516082704), ('स्टेन', 1, 0.3791907811537385), ('स्टेग', 1, 0.3308465126901865), ('स्टेफ', 1, 0.28260723769199103), ('स्टेज', 1, 0.28085588850080967), ('स्टोप', 1, 0.22106668446213007), ('स्टेल', 1, 0.20333348214626312), ('स्टेड', 1, 0.14664176013320684), ('स्टेउ', 1, 0.03653041273355484), ('स्टेस', 1, 0.003321461845189333), ('स्टेर', 1, -0.02870927471667528), ('स्टेच', 1, -0.053805332630872726), ('स्टिप', 1, -0.08760802634060383), ('स्टेभ', 1, -0.15846377331763506), ('स्टाप', 1, -0.20837458409368992)]\n",
      "[('भत्किएको', 0), ('भत्किएका', 1), ('भत्काएको', 1)]\n",
      "filtered candidates: [('भत्किएको', 0, 0.5456715356558561), ('भत्किएका', 1, 0.029336776584386826), ('भत्काएको', 1, -0.3194650635123253)]\n",
      "[('छैन', 0), ('छिन', 1), ('छुन', 1), ('छ्न', 1), ('छेन', 1), ('छान', 1), ('छउन', 1), ('छैै', 1), ('छैं', 1), ('छौन', 1), ('छैट', 1), ('छीन', 1), ('छैम', 1), ('छैँ', 1), ('छपन', 1), ('छंन', 1)]\n",
      "filtered candidates: [('छैन', 0, 1.2503018397837877), ('छीन', 1, 0.45400345697999), ('छैँ', 1, 0.40889943949878216), ('छैै', 1, 0.3359001036733389), ('छैम', 1, 0.32140285056084394), ('छ्न', 1, 0.3196005163481459), ('छौन', 1, 0.28971206955611706), ('छैं', 1, 0.23930269542324822), ('छपन', 1, 0.1846093093045056), ('छंन', 1, 0.16497435769997537), ('छान', 1, 0.1343934815376997), ('छेन', 1, -0.07116243243217468), ('छुन', 1, -0.1261809411807917), ('छिन', 1, -0.15026269108057022), ('छउन', 1, -0.17835651338100433), ('छैट', 1, -0.20001574652269483)]\n",
      "Corrected Sentence: नीति तथा कार्यक्रम प्राथमिकतामा उच्छ आर्थिक वृद्धि ‘हिलारी स्टेप भत्किएको छैन\n",
      "Context Similarities: {'नीति': 0.06669867, 'तथा': 0.08487356, 'कार्यक्रम': 0.0715154, 'प्राथमिकतामा': 0, 'उच्छ': -0.01170267, 'आर्थिक': 0.081207044, 'वृद्धि': 0.048649393, '‘हिलारी': 0, 'स्टेप': -0.023446789, 'भत्किएको': -0.056791056, 'छैन': 0.03128773}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def transliterate(word):\n",
    "    # Add your transliteration logic here\n",
    "    return word\n",
    "\n",
    "def generate_candidates(word, vocabulary, max_distance=1):\n",
    "    candidates = []\n",
    "    transliterated_word = transliterate(word)\n",
    "    first_char = transliterated_word[0]  # Get the first character of the transliterated word\n",
    "    for vocab_word in vocabulary:\n",
    "        transliterated_vocab_word = transliterate(vocab_word)\n",
    "        if transliterated_vocab_word.startswith(first_char):  # Check if first character matches\n",
    "            if len(transliterated_vocab_word)==len(transliterated_word):\n",
    "                distance = editdistance.eval(transliterated_word, transliterated_vocab_word)\n",
    "                if distance <= max_distance:\n",
    "                    candidates.append((vocab_word, distance))\n",
    "    return sorted(candidates, key=lambda x: x[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filter_by_context(candidates, context_words, model):\n",
    "    filtered_candidates = []\n",
    "    for candidate, distance in candidates:\n",
    "        candidate_score = 0\n",
    "        for context_word in context_words:\n",
    "            if context_word in model.key_to_index and candidate in model.key_to_index:\n",
    "                candidate_score += model.similarity(context_word, candidate)\n",
    "        filtered_candidates.append((candidate, distance, candidate_score))\n",
    "    \n",
    "    # Sort by score (descending) and distance (ascending)\n",
    "    filtered_candidates.sort(key=lambda x: (-x[2], x[1]))\n",
    "    return filtered_candidates if filtered_candidates else candidates\n",
    "\n",
    "def calculate_similarity_scores(words, model):\n",
    "    context_similarities = {}\n",
    "    for i, word in enumerate(words):\n",
    "        if word in model.key_to_index:\n",
    "            context_words = [w for j, w in enumerate(words) if j != i and w in model.key_to_index]\n",
    "            if context_words:\n",
    "                similarity_scores = [model.similarity(word, context_word) for context_word in context_words]\n",
    "                context_similarities[word] = np.mean(similarity_scores)\n",
    "            else:\n",
    "                context_similarities[word] = 0\n",
    "        else:\n",
    "            context_similarities[word] = 0  # Word not in the model\n",
    "    return context_similarities\n",
    "\n",
    "def correct_sentence(sentence, vocabulary, model, threshold=0.4):\n",
    "    words = sentence.split()\n",
    "    corrected_sentence = []\n",
    "    context_similarities = calculate_similarity_scores(words, model)\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if context_similarities.get(word, 0) < 0.3:\n",
    "            # Generate candidates for the word\n",
    "            candidates = generate_candidates(word, vocabulary)\n",
    "            print(candidates)\n",
    "            # Filter candidates based on context\n",
    "            filtered_candidates = filter_by_context(candidates, words, model)\n",
    "            # Select the best candidate (here, the first one after filtering)\n",
    "\n",
    "            print(f\"filtered candidates: {filtered_candidates}\")\n",
    "            best_candidate = filtered_candidates[0][0] if filtered_candidates else word\n",
    "            corrected_sentence.append(best_candidate)\n",
    "        else:\n",
    "            corrected_sentence.append(word)  # If word is in vocabulary and above threshold, use it as is\n",
    "            \n",
    "\n",
    "    return ' '.join(corrected_sentence), context_similarities\n",
    "\n",
    "\n",
    "# Test example\n",
    "erroneous_sentence =\"नीति तथा कार्यक्रम प्राथमिकतामा उच्छ आर्थिक वृद्धि ‘हिलारी स्टेप भत्किएको छैन\"#\" \"बाघले आफ्नो घर बनाए\"\n",
    "corrected_sentence, context_similarities = correct_sentence(erroneous_sentence, vocabulary, model, threshold=0.4)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)\n",
    "print(\"Context Similarities:\", context_similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['हार', 'धुनुहोस्', 'र', 'स्वस्थ', 'रहनुहोस्।']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"हार धुनुहोस् र स्वस्थ रहनुहोस्।\"\n",
    "words = sentence.split()\n",
    "print(words)\n",
    "corrected_sentence = []\n",
    "context_similarities = calculate_similarity_scores(words, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'हार': -0.004895555,\n",
       " 'धुनुहोस्': 0.062465362,\n",
       " 'र': 0,\n",
       " 'स्वस्थ': 0.0057145134,\n",
       " 'रहनुहोस्।': 0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on probaiblity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'key_to_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Test example\u001b[39;00m\n\u001b[0;32m     73\u001b[0m erroneous_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mहार धुनुहोस् र स्वस्थ रहनुहोस्।\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 75\u001b[0m corrected_sentence, context_similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcorrect_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43merroneous_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrected Sentence:\u001b[39m\u001b[38;5;124m\"\u001b[39m, corrected_sentence)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext Similarities:\u001b[39m\u001b[38;5;124m\"\u001b[39m, context_similarities)\n",
      "Cell \u001b[1;32mIn[26], line 54\u001b[0m, in \u001b[0;36mcorrect_sentence\u001b[1;34m(sentence, vocabulary, model, threshold)\u001b[0m\n\u001b[0;32m     52\u001b[0m words \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     53\u001b[0m corrected_sentence \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 54\u001b[0m context_similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_similarity_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(words):\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_similarities\u001b[38;5;241m.\u001b[39mget(word, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;66;03m# Generate candidates for the word\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[26], line 40\u001b[0m, in \u001b[0;36mcalculate_similarity_scores\u001b[1;34m(words, model)\u001b[0m\n\u001b[0;32m     38\u001b[0m context_similarities \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(words):\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_to_index\u001b[49m:\n\u001b[0;32m     41\u001b[0m         context_words \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m j, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(words) \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m!=\u001b[39m i \u001b[38;5;129;01mand\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mkey_to_index]\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m context_words:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'key_to_index'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.special import softmax\n",
    "\n",
    "def transliterate(word):\n",
    "    # Add your transliteration logic here\n",
    "    return word\n",
    "\n",
    "def generate_candidates(word, vocabulary, max_distance=1):\n",
    "    candidates = []\n",
    "    transliterated_word = transliterate(word)\n",
    "    first_char = transliterated_word[0]  # Get the first character of the transliterated word\n",
    "    for vocab_word in vocabulary:\n",
    "        transliterated_vocab_word = transliterate(vocab_word)\n",
    "        if transliterated_vocab_word.startswith(first_char):  # Check if first character matches\n",
    "            if len(transliterated_vocab_word) == len(transliterated_word):\n",
    "                distance = editdistance.eval(transliterated_word, transliterated_vocab_word)\n",
    "                if distance <= max_distance:\n",
    "                    candidates.append((vocab_word, distance))\n",
    "    return sorted(candidates, key=lambda x: x[1])\n",
    "\n",
    "def filter_by_context(candidates, context_words, model):\n",
    "    context_vectors = [model[context_word] for context_word in context_words if context_word in model.key_to_index]\n",
    "    filtered_candidates = []\n",
    "\n",
    "    for candidate, distance in candidates:\n",
    "        if candidate in model.key_to_index:\n",
    "            candidate_vector = model[candidate]\n",
    "            similarity_scores = [np.dot(candidate_vector, context_vector) for context_vector in context_vectors]\n",
    "            candidate_probability = np.mean(softmax(similarity_scores))  # Calculate the probability\n",
    "            filtered_candidates.append((candidate, distance, candidate_probability))\n",
    "\n",
    "    # Sort by probability (descending) and distance (ascending)\n",
    "    filtered_candidates.sort(key=lambda x: (-x[2], x[1]))\n",
    "    return filtered_candidates if filtered_candidates else candidates\n",
    "\n",
    "def calculate_similarity_scores(words, model):\n",
    "    context_similarities = {}\n",
    "    for i, word in enumerate(words):\n",
    "        if word in model.key_to_index:\n",
    "            context_words = [w for j, w in enumerate(words) if j != i and w in model.key_to_index]\n",
    "            if context_words:\n",
    "                similarity_scores = [model.similarity(word, context_word) for context_word in context_words]\n",
    "                context_similarities[word] = np.mean(similarity_scores)\n",
    "            else:\n",
    "                context_similarities[word] = 0\n",
    "        else:\n",
    "            context_similarities[word] = 0  # Word not in the model\n",
    "    return context_similarities\n",
    "\n",
    "def correct_sentence(sentence, vocabulary, model, threshold=0.4):\n",
    "    words = sentence.split()\n",
    "    corrected_sentence = []\n",
    "    context_similarities = calculate_similarity_scores(words, model)\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if context_similarities.get(word, 0) < 0.5:\n",
    "            # Generate candidates for the word\n",
    "            candidates = generate_candidates(word, vocabulary)\n",
    "            print(f\"Candidates: {candidates}\")\n",
    "            # Filter candidates based on context\n",
    "            filtered_candidates = filter_by_context(candidates, words, model)\n",
    "            print(f\"filtered candidates: {filtered_candidates}\")\n",
    "            # Select the best candidate (here, the first one after filtering)\n",
    "            best_candidate = filtered_candidates[0][0] if filtered_candidates else word\n",
    "            corrected_sentence.append(best_candidate)\n",
    "        else:\n",
    "            corrected_sentence.append(word)  # If word is in vocabulary and above threshold, use it as is\n",
    "\n",
    "    return ' '.join(corrected_sentence), context_similarities\n",
    "\n",
    "# Test example\n",
    "erroneous_sentence = \"हार धुनुहोस् र स्वस्थ रहनुहोस्।\"\n",
    "\n",
    "corrected_sentence, context_similarities = correct_sentence(erroneous_sentence, vocabulary, model, threshold=0.4)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)\n",
    "print(\"Context Similarities:\", context_similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the file\n",
    "# file_path = 'D:/Major Project/enironment/unique_words.txt'\n",
    "file_path = \"D:/Major Project/enironment/Cleaned_File/unique_words_json_kag_rabindra.xt\"\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    # Read all lines into a list\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Strip whitespace characters like `\\n` at the end of each line\n",
    "lines = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"C:/Users/nabin shrestha/Downloads/updated_Json_kag_rabindra_nepali_embeddings_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "723309"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the vocabulary and vectors from the existing model\n",
    "vocab = list(model.wv.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Sentence: म फुल टिप्छु\n",
      "Context Similarities: {'म': 0.35627684, 'फुल': 0.40660158, 'टिप्छु': 0.12126553}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Sample transliteration map (simplified for demonstration)\n",
    "transliteration_map = {\n",
    "    'अ': 'a', 'आ': 'aa', 'इ': 'i', 'ई': 'ee', 'उ': 'u', 'ऊ': 'oo', 'ऋ': 'ri', 'ए': 'e', 'ऐ': 'ai', 'ओ': 'o', 'औ': 'au',\n",
    "    'अं': 'am', 'अः': 'ah', 'ा': 'a', 'ि': 'i', 'ी': 'ee', 'ु': 'u', 'ू': 'oo', 'ृ': 'ri', 'े': 'e', 'ै': 'ai', 'ो': 'o', 'ौ': 'au',\n",
    "    'क': 'ka', 'ख': 'kha', 'ग': 'ga', 'घ': 'gha', 'ङ': 'nga', 'च': 'cha', 'छ': 'chha', 'ज': 'ja', 'झ': 'jha', 'ञ': 'nya',\n",
    "    'ट': 'ta', 'ठ': 'tha', 'ड': 'da', 'ढ': 'dha', 'ण': 'na', 'त': 'ta', 'थ': 'tha', 'द': 'da', 'ध': 'dha', 'न': 'na',\n",
    "    'प': 'pa', 'फ': 'pha', 'ब': 'ba', 'भ': 'bha', 'म': 'ma', 'य': 'ya', 'र': 'ra', 'ल': 'la', 'व': 'va', 'श': 'sha',\n",
    "    'ष': 'sha', 'स': 'sa', 'ह': 'ha', 'क्ष': 'kshya', 'त्र': 'tra', 'ज्ञ': 'gya', 'ऽ': \"'\", 'ँ': 'n', 'ं': 'm', 'ः': 'h',\n",
    "    '़': '', '्': '', '०': '0', '१': '1', '२': '2', '३': '3', '४': '4', '५': '5', '६': '6', '७': '7', '८': '8', '९': '9',\n",
    "    'ॐ': 'om', 'क़': 'qa', 'ख़': 'kha', 'ग़': 'gha', 'ज़': 'za', 'ड़': 'ra', 'ढ़': 'rha', 'फ़': 'fa', 'य़': 'ya'\n",
    "}\n",
    "\n",
    "\n",
    "def transliterate(word):\n",
    "    transliterated = ''.join(transliteration_map.get(char, char) for char in word)\n",
    "    # transliterated = word\n",
    "    return transliterated\n",
    "\n",
    "def generate_candidates(word, vocabulary, max_distance=1):\n",
    "    candidates = []\n",
    "    transliterated_word = transliterate(word)\n",
    "    first_char = transliterated_word[0]  # Get the first character of the transliterated word\n",
    "    for vocab_word in vocabulary:\n",
    "        transliterated_vocab_word = transliterate(vocab_word)\n",
    "        if transliterated_vocab_word.startswith(first_char):  # Check if first character matches\n",
    "            # if len(transliterated_vocab_word)==len(transliterated_word):\n",
    "              distance = editdistance.eval(transliterated_word, transliterated_vocab_word)\n",
    "              if distance <= max_distance:\n",
    "                  candidates.append((vocab_word, distance))\n",
    "    return sorted(candidates, key=lambda x: x[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filter_by_context(candidates, context_words, model):\n",
    "    filtered_candidates = []\n",
    "    for candidate, distance in candidates:\n",
    "        candidate_score = 0\n",
    "        for context_word in context_words:\n",
    "            if context_word in model.wv and candidate in model.wv:\n",
    "                candidate_score += model.wv.similarity(context_word, candidate)\n",
    "            # candidate_score = np.mean([candidate_score, 0])\n",
    "        filtered_candidates.append((candidate, distance, candidate_score))\n",
    "\n",
    "    # Sort by score (descending) and distance (ascending)\n",
    "    filtered_candidates.sort(key=lambda x: (-x[2], x[1]))\n",
    "    return filtered_candidates if filtered_candidates else candidates\n",
    "\n",
    "def calculate_similarity_scores(words, model):\n",
    "    context_similarities = {}\n",
    "    for i, word in enumerate(words):\n",
    "        if word in model.wv:\n",
    "            context_words = [w for j, w in enumerate(words) if j != i and w in model.wv]\n",
    "            if context_words:\n",
    "                similarity_scores = [model.wv.similarity(word, context_word) for context_word in context_words]\n",
    "                context_similarities[word] = np.mean(similarity_scores)\n",
    "            else:\n",
    "                context_similarities[word] = 0\n",
    "        else:\n",
    "            context_similarities[word] = 0  # Word not in the model\n",
    "    return context_similarities\n",
    "\n",
    "def correct_sentence(sentence, vocabulary, model, threshold=0.4):\n",
    "    words = sentence.split()\n",
    "    corrected_sentence = []\n",
    "    context_similarities = calculate_similarity_scores(words, model)\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if context_similarities.get(word, 0) < 0.1:\n",
    "            # Generate candidates for the word\n",
    "            candidates = generate_candidates(word, vocabulary)\n",
    "            print(candidates)\n",
    "            # Filter candidates based on context\n",
    "            filtered_candidates = filter_by_context(candidates, words, model)\n",
    "            # Select the best candidate (here, the first one after filtering)\n",
    "\n",
    "            print(f\"filtered candidates: {filtered_candidates}\")\n",
    "            best_candidate = filtered_candidates[0][0] if filtered_candidates else word\n",
    "            corrected_sentence.append(best_candidate)\n",
    "        else:\n",
    "            corrected_sentence.append(word)  # If word is in vocabulary and above threshold, use it as is\n",
    "\n",
    "\n",
    "    return ' '.join(corrected_sentence), context_similarities\n",
    "\n",
    "\n",
    "# Test example\n",
    "erroneous_sentence =\"म फुल टिप्छु\"\n",
    "corrected_sentence, context_similarities = correct_sentence(erroneous_sentence, vocab, model, threshold=0.6)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)\n",
    "print(\"Context Similarities:\", context_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"म\" in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spell_checker_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
